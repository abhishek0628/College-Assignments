{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c4d5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8ee493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=pd.read_csv('train_X.csv')\n",
    "y_train=pd.read_csv('train_label.csv').to_numpy()\n",
    "x_test=pd.read_csv('test_X.csv')\n",
    "y_test=pd.read_csv('test_label.csv').to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2008847-4a7a-4fcb-af93-66709418fb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "274bcd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train / 255.0\n",
    "x_test=x_test /255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc103ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e932e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4670dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    caches = {}\n",
    "    A = X.T\n",
    "    L = len(parameters) // 2 \n",
    "    \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        Z = parameters['W' + str(l)].dot(A) + parameters['b' + str(l)]\n",
    "        A = relu(Z)\n",
    "        caches['A' + str(l)] = A\n",
    "        caches['Z' + str(l)] = Z\n",
    "\n",
    "    \n",
    "    ZL = parameters['W' + str(L)].dot(A) + parameters['b' + str(L)]\n",
    "    AL = softmax(ZL)\n",
    "    caches['A' + str(L)] = AL\n",
    "    caches['Z' + str(L)] = ZL\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60b83483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AL, Y):\n",
    "    m = Y.shape[0]\n",
    "    cost = -np.sum(Y.T * np.log(AL + 1e-8)) / m\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "921864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, caches, parameters):\n",
    "    grads = {}\n",
    "    L = len(parameters) // 2\n",
    "    m = X.shape[0]\n",
    "    Y = Y.T\n",
    "    A_prev = X.T\n",
    "\n",
    "    \n",
    "    dZL = caches['A' + str(L)] - Y\n",
    "    grads['dW' + str(L)] = dZL.dot(caches['A' + str(L-1)].T) / m\n",
    "    grads['db' + str(L)] = np.sum(dZL, axis=1, keepdims=True) / m\n",
    "\n",
    "   \n",
    "    for l in reversed(range(1, L)):\n",
    "        dA = parameters['W' + str(l+1)].T.dot(dZL)\n",
    "        dZ = dA * relu_derivative(caches['Z' + str(l)])\n",
    "        A_prev = X.T if l == 1 else caches['A' + str(l-1)]\n",
    "        grads['dW' + str(l)] = dZ.dot(A_prev.T) / m\n",
    "        grads['db' + str(l)] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        dZL = dZ\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25e836c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L + 1):\n",
    "        parameters['W' + str(l)] -= learning_rate * grads['dW' + str(l)]\n",
    "        parameters['b' + str(l)] -= learning_rate * grads['db' + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5be9c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, layer_dims, learning_rate=0.01, num_epochs=100):\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    for epoch in range(num_epochs):\n",
    "       \n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "       \n",
    "        cost = cost_function(AL, Y)\n",
    "        \n",
    "        \n",
    "        grads = backward_propagation(X, Y, caches, parameters)\n",
    "        \n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Cost after epoch {epoch}: {cost:.4f}\")\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3ae501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.3034\n",
      "Cost after epoch 10: 2.2745\n",
      "Cost after epoch 20: 2.2233\n",
      "Cost after epoch 30: 2.1222\n",
      "Cost after epoch 40: 1.9478\n",
      "Cost after epoch 50: 1.7055\n",
      "Cost after epoch 60: 1.4438\n",
      "Cost after epoch 70: 1.2163\n",
      "Cost after epoch 80: 1.0402\n",
      "Cost after epoch 90: 0.9072\n",
      "Cost after epoch 100: 0.8056\n",
      "Cost after epoch 110: 0.7261\n",
      "Cost after epoch 120: 0.6622\n",
      "Cost after epoch 130: 0.6098\n",
      "Cost after epoch 140: 0.5660\n",
      "Cost after epoch 150: 0.5288\n",
      "Cost after epoch 160: 0.4968\n",
      "Cost after epoch 170: 0.4688\n",
      "Cost after epoch 180: 0.4442\n",
      "Cost after epoch 190: 0.4222\n",
      "Cost after epoch 200: 0.4025\n",
      "Cost after epoch 210: 0.3846\n",
      "Cost after epoch 220: 0.3683\n",
      "Cost after epoch 230: 0.3532\n",
      "Cost after epoch 240: 0.3393\n",
      "Cost after epoch 250: 0.3264\n",
      "Cost after epoch 260: 0.3144\n",
      "Cost after epoch 270: 0.3031\n",
      "Cost after epoch 280: 0.2924\n",
      "Cost after epoch 290: 0.2824\n",
      "Cost after epoch 300: 0.2729\n",
      "Cost after epoch 310: 0.2639\n",
      "Cost after epoch 320: 0.2553\n",
      "Cost after epoch 330: 0.2472\n",
      "Cost after epoch 340: 0.2394\n",
      "Cost after epoch 350: 0.2319\n",
      "Cost after epoch 360: 0.2248\n",
      "Cost after epoch 370: 0.2180\n",
      "Cost after epoch 380: 0.2114\n",
      "Cost after epoch 390: 0.2051\n",
      "Cost after epoch 400: 0.1990\n",
      "Cost after epoch 410: 0.1932\n",
      "Cost after epoch 420: 0.1876\n",
      "Cost after epoch 430: 0.1822\n",
      "Cost after epoch 440: 0.1770\n",
      "Cost after epoch 450: 0.1720\n",
      "Cost after epoch 460: 0.1672\n",
      "Cost after epoch 470: 0.1625\n",
      "Cost after epoch 480: 0.1580\n",
      "Cost after epoch 490: 0.1537\n",
      "Cost after epoch 500: 0.1494\n",
      "Cost after epoch 510: 0.1454\n",
      "Cost after epoch 520: 0.1414\n",
      "Cost after epoch 530: 0.1376\n",
      "Cost after epoch 540: 0.1339\n",
      "Cost after epoch 550: 0.1303\n",
      "Cost after epoch 560: 0.1269\n",
      "Cost after epoch 570: 0.1235\n",
      "Cost after epoch 580: 0.1203\n",
      "Cost after epoch 590: 0.1171\n",
      "Cost after epoch 600: 0.1141\n",
      "Cost after epoch 610: 0.1111\n",
      "Cost after epoch 620: 0.1082\n",
      "Cost after epoch 630: 0.1054\n",
      "Cost after epoch 640: 0.1027\n",
      "Cost after epoch 650: 0.1001\n",
      "Cost after epoch 660: 0.0976\n",
      "Cost after epoch 670: 0.0951\n",
      "Cost after epoch 680: 0.0927\n",
      "Cost after epoch 690: 0.0904\n",
      "Cost after epoch 700: 0.0882\n",
      "Cost after epoch 710: 0.0860\n",
      "Cost after epoch 720: 0.0839\n",
      "Cost after epoch 730: 0.0818\n",
      "Cost after epoch 740: 0.0799\n",
      "Cost after epoch 750: 0.0779\n",
      "Cost after epoch 760: 0.0761\n",
      "Cost after epoch 770: 0.0743\n",
      "Cost after epoch 780: 0.0725\n",
      "Cost after epoch 790: 0.0709\n",
      "Cost after epoch 800: 0.0692\n",
      "Cost after epoch 810: 0.0676\n",
      "Cost after epoch 820: 0.0661\n",
      "Cost after epoch 830: 0.0646\n",
      "Cost after epoch 840: 0.0632\n",
      "Cost after epoch 850: 0.0618\n",
      "Cost after epoch 860: 0.0605\n",
      "Cost after epoch 870: 0.0592\n",
      "Cost after epoch 880: 0.0579\n",
      "Cost after epoch 890: 0.0567\n",
      "Cost after epoch 900: 0.0555\n",
      "Cost after epoch 910: 0.0543\n",
      "Cost after epoch 920: 0.0532\n",
      "Cost after epoch 930: 0.0521\n",
      "Cost after epoch 940: 0.0511\n",
      "Cost after epoch 950: 0.0501\n",
      "Cost after epoch 960: 0.0491\n",
      "Cost after epoch 970: 0.0481\n",
      "Cost after epoch 980: 0.0472\n",
      "Cost after epoch 990: 0.0463\n",
      "Train accuracy: 99.90%\n",
      "Test accuracy: 85.67%\n"
     ]
    }
   ],
   "source": [
    "def predict(X, parameters):\n",
    "    AL, _ = forward_propagation(X, parameters)\n",
    "    predictions = np.argmax(AL, axis=0)\n",
    "    return predictions\n",
    "\n",
    "def accuracy(predictions, Y):\n",
    "    true_labels = np.argmax(Y, axis=1)\n",
    "    return np.mean(predictions == true_labels)\n",
    "\n",
    "# x_test_first=x_test.iloc[0,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layer_dims = [784,256, 10]\n",
    "\n",
    "\n",
    "\n",
    "parameters = train_model(x_train, y_train, layer_dims, learning_rate=0.1, num_epochs=1000)\n",
    "\n",
    "\n",
    "train_preds = predict(x_train, parameters)\n",
    "test_preds = predict(x_test, parameters)\n",
    "\n",
    "print(f\"Train accuracy: {accuracy(train_preds, y_train) * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {accuracy(test_preds, y_test) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07c5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf2a1d-d358-4629-b722-da8a4dee5e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055a141-cd39-4578-88bc-cb55026e6e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c36e8-4e5a-4698-993b-a2d165e06394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0405a-9f9a-42b9-9931-6f1e587ffa69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
